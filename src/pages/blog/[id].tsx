import Header from '@/components/Header';
import SubTitle from '@/components/SubTitle';
import TextBlog from '@/components/TextBlog';
import TitleBlog from '@/components/TitleBlog';
import { useRouter } from 'next/router';

const BlogPost = () => {
    const router = useRouter();
    const { id } = router.query;

    return (
        <div className='text-black'>
            <Header />
            <SubTitle title="We are developing adjustable ML models for better performance and fairness"/>
            <TitleBlog 
                title="Fostering Fairness in Medical Image Datasets: Identification of Protected Attributes Through Foundation Models"
                date="21 January 2024"
                author="By: Dilermando Queiroz Neto"
            />
            <TextBlog
                text="Ensuring that models deliver consistent performance across diverse population segments is essential for advancing medical image diagnostics. Incorporating fairness considerations into machine learning models is crucial to address biases and promote equitable healthcare systems. However, many databases do not provide protected attributes or contain unbalanced representations of demographic groups. This study sets out to investigate the usefulness of the backbone of Foundation Models as an embedding extractor to create groups representing different protected attributes, such as gender and age, and to sample data for this cluster to create a new fairer subset to utilize in fine-tuning. Using databases in and out-of-distribution scenarios, it is possible to identify that the method can create groups that represent different protected attributes in both databases and reduce in 4.44\% the difference between the gender attribute in-distribution and 6.16\% in out-of-distribution, creating a fairer subset. These findings suggest a role in promoting fairness assessment in scenarios where we lack knowledge of attributes, contributing to the development of more equitable medical diagnostics.<br/><br/>In recent years, we have witnessed remarkable advancements in medicine, particularly in medical diagnosis, with the integration of Deep Learning (DL) techniques and the evolution of cloud computing. This technological revolution holds the potential to enhance diagnostic accuracy and make medical services more accessible. An example is the deployment of cloud-based systems integrating DL techniques \cite{widner_lessons_2023}, which underscores the potential for broader accessibility across numerous hospitals, streamlining the diagnostic process and providing an invaluable tool for medical professionals.<br/><br/>As DL algorithms rapidly proliferate in the healthcare sector, ethical concerns regarding their impact on underrepresented communities have emerged \cite{EthicalChallenges, mccradden_ethical_2020}. Studies have indicated that AI algorithms can uncover causal structures in data that correlate with protected identity status. In the analysis of medical images, these correlations might be wrongly used to infer protected characteristics such as race, gender, age, and ethnicity \cite{yi_radiology_2021, glocker_algorithmic_2023}. Such correlations suggest that DL algorithms could potentially use protected identity statuses as a shortcut to predict health outcomes, posing a risk of exacerbating existing healthcare inequalities that disproportionately affect vulnerable populations.<br/><br/>To address this pressing issue, a variety of techniques aimed at mitigating algorithmic bias have been proposed and critically evaluated in literature~\cite{huang_evaluation_2022, chen_algorithmic_2023, ricci_lara_addressing_2022, survey_bias}. These methods span pre-processing, in-processing, and post-processing stages and are recommended by global health authorities like the World Health Organization \cite{10665-373421} to foster equity in healthcare. Such mitigation strategies must also be applied to novel domains within the field, such as in developing and using the Foundation Models (FM)~\cite{bommasani2022opportunities}.<br/> <br/>A common issue with medical image data is that many publicly available medical image datasets do not provide demographic information. For example, in chest X-ray datasets, only a few include protected attributes~\cite{Gichoya2022}, which makes it challenging to evaluate DL models trained on such data across different demographics and sensitive variables. Moreover, the vast majority of fairness techniques require datasets containing this information~\cite{huang_evaluation_2022, chen_algorithmic_2023, survey_bias}. Therefore, techniques to mitigate unfair treatment from DL models that would not rely on these attributes are essential for the development of the field.<br/><br/>The introduction of new DL techniques, such as self-supervised learning, represents a significant leap forward. The use of more images without the need for specific labels, as discussed in research on self-supervised learning (SSL) in healthcare~\cite{Huang2023}, allows models to be trained with large quantities of unannotated data, bypassing expensive and tedious labeling processes. Furthermore, SSL can help mitigate biases in datasets by learning more general representations that are less dependent on labeled data, which often contain human biases.<br/><br/>Projects utilizing these techniques have been developed recently, such as the case of Robust and Efficient MEDical Imaging with Self-supervision . According to the results obtained by the authors, the developed model showed a significant improvement in in-distribution diagnostics by 11.5\% and required only 1-33\% of the data for out-of-distribution to achieve the same performance as supervised models using all the data. With these results, it is possible to use FM in different production environments.<br/><br/>However, it is essential to note that challenges persist even with these advancements. Recent papers exploring issues of bias in FM indicate that the pursuit of equity still needs to be fully resolved. Underscores the ongoing need for research and improvement in fairness in FM \cite{AZIZI2023CJ}. Such models require a vast amount of data for training, which can introduce biases towards under-represented populations.<br/><br/>The primary focus of this research is to use the backbone of an FM to create groups that can be used in fairness evaluation techniques in datasets lacking sensitive metadata. Our main goal is not to remove the need for sensitive data but to enable evaluation in scenarios where such data may not be available. Furthermore, this study explores techniques for creating more equitable datasets for fine-tuning FM. We use the findings of to achieve a more equitable data reduction than random methods. Figure details the methodology and process."
            />
        </div>
    );
};

export default BlogPost;
